# Agentic RAG â€“ Dynamic Knowledge Companion

A productionâ€‘ready reference implementation of an **Agentic Retrievalâ€‘Augmented Generation** system that adapts to query type and fuses **local knowledge**, **live web results**, and **optional external tools/APIs**. The assistant behaves like a **dynamic knowledge companion**: plans, retrieves, reasons, verifies, and cites.

---

## âœ¨ Highlights

* **Multiâ€‘agent architecture**: Router â†’ Planner â†’ Retriever(s) â†’ Synthesizer â†’ Verifier â†’ Finalizer.
* **Hybrid retrieval**: FAISS over your local docs + live web search (DuckDuckGo, Tavily, or SerpAPI).
* **Dynamic strategies** by query type: factual, exploratory, reasoningâ€‘heavy, and data/tool tasks.
* **Transparent outputs**: citations, source rationale, and a trace of the reasoning steps.
* **Lowâ€‘latency defaults** with optional reâ€‘ranking and crossâ€‘validation passes.

---

## ğŸ—‚ï¸ Repo Layout

```
agentic-rag/
â”œâ”€ README.md                        # This file
â”œâ”€ requirements.txt
â”œâ”€ .env.example
â”œâ”€ data/
â”‚  â””â”€ sample_docs/                  # Put your PDFs/CSVs/TXTs/MDs here
â”œâ”€ app/
â”‚  â”œâ”€ main.py                       # CLI & simple REST server
â”‚  â”œâ”€ config.py                     # Settings & feature flags
â”‚  â”œâ”€ schema.py                     # Pydantic models for I/O contracts
â”‚  â”œâ”€ orchestrator.py               # High-level pipeline orchestration
â”‚  â”œâ”€ agents/
â”‚  â”‚  â”œâ”€ router.py                  # Query type classifier
â”‚  â”‚  â”œâ”€ planner.py                 # Step planner
â”‚  â”‚  â”œâ”€ retriever.py               # Multi-source retrieval coordinator
â”‚  â”‚  â”œâ”€ synthesizer.py             # Answer generation with citations
â”‚  â”‚  â”œâ”€ verifier.py                # Cross-check critical claims
â”‚  â”‚  â””â”€ finalizer.py               # UX formatting & transparency bundle
â”‚  â”œâ”€ retrievers/
â”‚  â”‚  â”œâ”€ local_index.py             # FAISS + sentence-transformers
â”‚  â”‚  â”œâ”€ web_search.py              # DuckDuckGo/Tavily/SerpAPI adapters
â”‚  â”‚  â””â”€ reranker.py                # (Optional) Cross-encoder reranker
â”‚  â”œâ”€ tools/
â”‚  â”‚  â”œâ”€ calculator.py              # Safe eval math tool
â”‚  â”‚  â”œâ”€ code_exec_stub.py          # (Optional) sandboxes for code
â”‚  â”‚  â””â”€ example_api.py             # (Optional) external API helper
â”‚  â””â”€ utils/
â”‚     â”œâ”€ io.py                      # Loader for PDF, MD, TXT, CSV
â”‚     â”œâ”€ text.py                    # Chunking, cleaning
â”‚     â”œâ”€ llm.py                     # LLM abstraction (OpenAI/LiteLLM)
â”‚     â”œâ”€ citations.py               # Citation formatting & de-dup
â”‚     â””â”€ trace.py                   # Structured logging
â””â”€ tests/
   â””â”€ smoke_tests.py                # Minimal e2e validation
```

---

## ğŸ”§ Setup

1. **Python**: 3.10+
2. **Install**:

   ```bash
   pip install -r requirements.txt
   ```
3. **Environment**: copy & edit `.env.example` â†’ `.env` (put keys if youâ€™ll use LLM APIs or web providers):

   ```env
   OPENAI_API_KEY=...
   LLM_PROVIDER=openai   # or azure_openai, groq, anthropic via litellm
   EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

   # Web search (pick any):
   TAVILY_API_KEY=...
   SERPAPI_API_KEY=...
   SERPER_API_KEY=...
   WEB_PROVIDER=duckduckgo  # duckduckgo | tavily | serpapi | serper

   # Optional: Reranker
   RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
   ```
4. **Index local docs** (place files in `data/sample_docs/`), then:

   ```bash
   python -m app.main --reindex
   ```

---

## â–¶ï¸ Run

### A) CLI

```bash
python -m app.main --ask "What are the key risks mentioned in the quarterly PDF?"
```

### B) Simple REST (FastAPI)

```bash
python -m app.main --serve --host 0.0.0.0 --port 8000
```

Then POST to `/ask` with JSON:

```json
{
  "query": "Summarize the latest findings on X and contrast with our local report",
  "options": {"use_web": true, "max_tokens": 800}
}
```

---

## ğŸ§  Agent Workflow (at a glance)

1. **Router**: classify query â†’ `{factual|open|reasoning|tool}` and select strategy knobs (k, web\_on, depth).
2. **Planner**: create a short plan (steps & tools).
3. **Retriever**: parallel local index search + web search; optional re-ranking; deduplicate; normalize citations.
4. **Synthesizer**: instruct LLM to **ground generations** on retrieved chunks; produce draft with inline cite markers `[S1]`â€¦
5. **Verifier**: extract critical claims â†’ corroborate via second-pass web checks; flag inconsistencies â†’ feedback loop.
6. **Finalizer**: compose answer + **Sources**, **Why these sources**, **Trace**.

---

## âœ… Evaluation (quick smoke tests)

```bash
pytest -q
```

* **Answer quality**: basic assertions (has citations, no empty sections).
* **Adaptability**: router labels & plan contain expected stages.
* **Grounding**: every claim block references a retrieved source id.

---

## ğŸ“ Code

### requirements.txt

```txt
fastapi==0.114.2
uvicorn==0.30.6
pydantic==2.9.2
python-dotenv==1.0.1
faiss-cpu==1.8.0.post1
sentence-transformers==3.0.1
numpy==1.26.4
pandas==2.2.2
scikit-learn==1.5.1
duckduckgo-search==6.1.7
httpx==0.27.2
rich==13.8.1
orjson==3.10.7
jinja2==3.1.4
# Optional LLMs (choose one path):
litellm==1.48.1   # universal LLM adapter (OpenAI/Anthropic/Groq/Azure)
```